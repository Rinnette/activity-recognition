---
title: "Predicting how well a weight lifting exercise is performed"
author: "Rinnette N. Ramdhanie"
date: "29 January 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadLibs, warning = FALSE, message = FALSE, echo = FALSE}
        library(caret)
        library(corrplot)
```

## Overview

Data from research done in human activity recognition was used in this exercise to determine "how well" a group of participants performed the Unilateral Dumbbell Biceps Curl.  They performed 10 repetitions in 5 different ways: according to specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The data contains readings taken from the participants while they were performing the activity as described.  This project trains a model to predict, given a set of readings, which class a particular repetition belongs to.


## Loading the data

The data used is the Weight Lifting Exercises dataset obtained from Human Activity Recognition (HAR) research and is located on this website: http://groupware.les.inf.puc-rio.br/har.  The training and testing datasets were downloaded as csv files, then loaded into R.

```{r loaddata}
        training <- read.csv("../pml-training.csv")
        testing <- read.csv("../pml-testing.csv")
```

## Exploratory analysis & Data processing
The structure of the dataset was checked as well as the dimensions of the training and testing datasets.

```{r showstr, results = "hide"}
        # Show the structure of the dataset: name and class of each column
        str(training)
```

```{r showdim}
        # Show dimensions of datasets
        dim(training); dim(testing)
```

There are 160 variables which need to be examined to determine if all the variables should be used in the models.  Both the training and testing dataset were processed as follows:

1. The first seven columns of the datasets seem unnecessary for predicting the outcome as they contain information such as the names of the persons involved in the exercise and time stamps. 

```{r rmFirst7}
        # Remove first 7 columns
        train <- training[, -c(1:7)]; test <- testing[, -c(1:7)]
```

2. Columns with little or no variance were removed since these columns will have little or no variation as they consist mainly of a single value.

```{r rmNearZero}
        # Remove columns with near zero variance
        train <- train[ , -nearZeroVar(train)]
        test <- test[ , -nearZeroVar(test)]
```

3. Some columns contained more than 97% NAs even after removal of near zero variance columns. The number of NAs 

```{r checkNAs, results = "hide"}
        # Check sum of NAs in each column
                checkNA <- capture.output(colSums(is.na(train)))
                checkNA
```

It was noted that all the columns with NAs contain the same amount of NAs, ie. 19 216, which is over 97% of the total number of values in the column.  These columns were removed instead of replacing the NAs with 0's, as the presence of so many columns with 0's may skew the model.

```{r rmNAs}
        # Remove columns with more than 90% NA's
        train <- train[ , which(colMeans(is.na(train)) < 0.9)]
        test <- test[ , which(colMeans(is.na(test)) < 0.9)]
```

The number of variables has been reduced to 53 including the outcome variable *classe*.

The correlation among the remaining variables was then checked to determine whether the number of variables could be further reduced.  The *classe* variable was not included in the correlation matrix created.  The values obtained are plotted in the figure below.  The darkest values show correlation close to 1 with blue representing positive correlation and red representing negative correlation.

```{r corrPlot, fig.width=7, fig.height=7}
        corrMatrix <- cor(train[, -53])
        corrplot(corrMatrix, method = "color", type = "lower", order = "AOE")

```

The plot shows that several variables are highly correlated therefore the preprocessing method, Principle Component Analysis (PCA) was used to combine highly correlated variables thus reducing the number of variables even further.  Using PCA can also help to reduce overfitting and computations are easier (faster) with less variables.  Note however that when variables are combined some information is lost and this can affect the accuracy of the classification model.  The models below were therefore fitted using both the data that was preprocessed, as well as the data that was not preprocessed to compare the accuracy figures obtained.

## Partition the **train** dataset
The **train** dataset was partitioned to get a new training set (**trainset**) and a validation set (**validset**).  The **test** dataset was held back to be used with the final model at the end of the exercise. 

```{r dataPartition}
        inTrain <- createDataPartition(train$classe, p = 0.70, list = FALSE)
        trainset <- train[inTrain, ]
        validset <- train[-inTrain, ]
```

## Cross Validation
A 5-fold cross validation method was applied when training the model.

```{r setTrainCtrl}
        traincontrol <- trainControl(method = "cv", number = 5)
```

## Prediction models to be used
The following models were fitted and the one with the highest accuracy was selected for the final predictions.

* Random forest
* Boosting
* Classification tree


## Model fitting and selection
PCA was applied to **trainset** and **validset**. The number of variables were reduced to 26 including the outcome variable *classe*.

```{r preProcSets}
        preProctrain <- preProcess(trainset[,-53], method = "pca")
        trainPP <- predict(preProctrain, trainset)
        preProcvalid <- preProcess(validset[,-53], method = "pca")
        validPP <- predict(preProcvalid, validset)
```

### Random forest
The random forest algorithm was applied to both the preprocessed data as well as the unprocessed data.

```{r rfmodel}
        set.seed(999)
        # RFtrainPP <- train(classe ~ ., data = trainPP, method = "rf", trControl = traincontrol)
        # RFtrain <- train(classe ~ ., data = trainset, method = "rf", trControl = traincontrol)
        # RFtrainPP
        # RFtrain
```

The accuracy with the unprocessed data was higher at 0.99 compared to the preprocessed data at 0.97.  The random forest model obtained with the unprocessed data was then applied to the validation set.

```{r predvalid}
        # predvalidRF <- predict(RFtrain, validset)
        # confusionMatrix(predvalidRF, validset$classe)
```

An accuracy value of over 99% was obtained.  The other models, however, were still checked for comparison.
 
### Boosting
```{r gbmmodel, results = "hide"}
        # set.seed(999)
        # GBMtrainPP <- train(classe ~ ., data = trainPP, method = "gbm", trControl = traincontrol)
        # GBMtrain <- train(classe ~ ., data = trainset, method = "gbm", trControl = traincontrol)
        # GBMtrainPP
        # GBMtrain
```

The accuracy with the unprocessed data was higher at 0.96 compared to the preprocessed data at 0.82.  This is close to the accuracy obtained for random forest but still less so it was not considered.

### Classification tree
```{r ctmodel, results = "hide"}
        # set.seed(999)
        # CTtrainPP <- train(classe ~ ., data = trainPP, method = "rpart", trControl = traincontrol)
        # CTtrain <- train(classe ~ ., data = trainset, method = "rpart", trControl = traincontrol)
        # CTtrainPP
        # CTtrain
```

The accuracy with the unprocessed data was higher at 0.50 compared to the preprocessed data at 0.42. This is much less than the accuracy for random forest and was not considered.

### Model selection
Generally, the accuracy for models fitted using the preprocessed data was smaller than for the corresponding unprocessed data probably because information was lost when the variables were combined. 

The random forest model was therefore selected with no preprocessing.

## Test data
The selected model was applied to the test data.

```{r predtest}
        #predict(RFtrain, test)
```


## Discussion




## References
1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

